# vLLM generation settings
use_vllm: true
vllm_mode: "server"

# whether completions are multi-turn or single-turn
multi_turn: false
# Tool responses contain "unpredictable" tokens we don't want to include in the loss
mask_tool_responses: false

# Optimizer settings
learning_rate: 1e-5  # default value is for non-lora 
adam_beta1: 0.9
adam_beta2: 0.99
weight_decay: 0.1
warmup_ratio: 0.1
lr_scheduler_type: "constant_with_warmup"  # cosine is empirically bad for GRPO
optim: "paged_adamw_8bit"

# Model settings
bf16: true
fp16: false

# Generation and Training settings
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
num_generations: 4
max_prompt_length: 256
max_completion_length: 256

# Reward settings
scale_rewards: false
# Loss type
loss_type: "grpo"
# Attention kernel
use_liger_loss: true

# Gradient checkpointing
gradient_checkpointing: false

# Training loop settings
logging_steps: 1
max_steps: -1
save_steps: 50
max_grad_norm: 0.1

# Logging settings
report_to: "wandb"
run_name: "${model.model_name}-GRPO-${run.dataset_type}-${run.task_type}-${now:}"
output_dir: "outputs"
log_completions: true

# silence peft warnings
label_names:
  - "labels" 