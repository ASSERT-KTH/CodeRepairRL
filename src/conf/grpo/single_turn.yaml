# vLLM generation settings
use_vllm: true
vllm_mode: "server"
vllm_server_host: "127.0.0.1"

# whether completions are multi-turn or single-turn
multi_turn: false
# Tool responses contain "unpredictable" tokens we don't want to include in the loss
mask_tool_responses: false

# Optimizer settings
learning_rate: 1e-4  # default value is for lora 
adam_beta1: 0.9
adam_beta2: 0.99
weight_decay: 0.1
warmup_ratio: 0.1
lr_scheduler_type: "constant_with_warmup"  # cosine is empirically bad for GRPO
optim: "paged_adamw_8bit"

# Model settings
bf16: true
fp16: false
disable_dropout: true

# Generation and Training settings
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
num_generations: 4
generation_batch_size: 4
max_prompt_length: 256
max_completion_length: 256

# KL to a frozen version (unmerged LoRA) with this coef
beta: 0.02

# Reward settings
scale_rewards: false
# Loss type
loss_type: "grpo"
# Attention kernel
use_liger_loss: true

# Gradient checkpointing
gradient_checkpointing: false

# Training loop settings
num_train_epochs: 3
max_steps: -1
save_steps: 20
logging_steps: 1
save_total_limit: 100  # LoRAs are small
max_grad_norm: 0.1  # replacing with advantage estimation empirically has higher variance
resume_from_checkpoint: null

# Logging settings
report_to: "wandb"
run_name: ???  # required at runtime
log_completions: true

# silence peft warnings
label_names:
  - "labels" 

ddp_find_unused_parameters: false  # Safe when working on dense LLMs, MoE would be problematic
ddp_bucket_cap_mb: 16