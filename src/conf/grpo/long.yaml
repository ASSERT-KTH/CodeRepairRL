defaults:
  - short

vllm_mode: "async_server"

per_device_train_batch_size: 4
gradient_accumulation_steps: 1
num_generations: 4
max_prompt_length: 1024  # not strictly enforced
max_completion_length: 7168  # but dr_grpo uses this for normalization

multi_turn: true
mask_tool_responses: true

loss_type: "dr_grpo"  # better for long sequences

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true
