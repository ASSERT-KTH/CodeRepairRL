defaults:
  - single_turn

vllm_mode: "async_server"
multi_turn: true
mask_tool_responses: false

max_prompt_length: 1024  # not strictly enforced
max_completion_length: 7168  # but dr_grpo uses this for normalization

# we must fit an entire group per forward pass (per_device_train_batch_size * num_gpus % num_generations == 0)
num_generations: 8  # group size of GRPO
per_device_train_batch_size: 4  # "default" runs have 2 training GPUs 
gradient_accumulation_steps: 4

loss_type: "dr_grpo"

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true