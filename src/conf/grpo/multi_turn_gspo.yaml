defaults:
  - multi_turn

# GSPO with no modifications and our custom Liger Kernel
loss_type: grpo  # needs to be this value to actually work as GSPO
importance_sampling_level: "sequence"  # GRPO assigns importance weights per token, which is very unstable
mask_tool_responses: false
scale_rewards: true

# we must fit an entire group per forward pass (per_device_train_batch_size * num_gpus % num_generations == 0)
num_generations: 8  # group size of GRPO
generation_batch_size: 8
per_device_train_batch_size: 4  # "default" runs have 2 training GPUs, 2x4=8
gradient_accumulation_steps: 4
num_iterations: 1  # we cache the generated rollouts and update on them twice (this enables the clipping)

# GSPO describes having no KL, but a small beta helps stabilize our multi-turn / long context runs
beta: 0.02

# clipping
epsilon: 2e-5
epsilon_high: 4e-4