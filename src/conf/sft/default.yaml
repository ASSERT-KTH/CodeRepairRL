# TrainingArguments parameters
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 8

learning_rate: 1e-4  # default value is for lora
warmup_ratio: 0.1
lr_scheduler_type: "cosine"
max_grad_norm: 0.5 
weight_decay: 0.01

# Model settings
bf16: true
fp16: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true

ddp_find_unused_parameters: false  # Safe when working on dense LLMs, MoE would be problematic
ddp_bucket_cap_mb: 16

# Training loop settings
logging_steps: 1
save_steps: 100
save_total_limit: 5
eval_steps: 100000000

# Logging
report_to: "wandb"
run_name: ???

max_length: 16384
packing: false  # pack two shorter rollouts into one to match a longer, max_length rollout

assistant_only_loss: false  # ideally this would be true, but Qwen3's chat template does not support it

# kl_lambda, used in compute_loss_func, popped to match SFTConfig
kl_lambda: 0.05
