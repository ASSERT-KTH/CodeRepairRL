# Model configuration
model_name: "Qwen/Qwen2.5-Coder-1.5B-Instruct"
attn_implementation: "flash_attention_2"
load_in_8bit: true
# LoRA configuration
lora: true
# only used if run.lora is true
r: 32
lora_alpha: 64  # lora paper describes 2x the r
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"