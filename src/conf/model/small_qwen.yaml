# Model configuration
model_name: "Qwen/Qwen3-8B"
lora_checkpoint_path: null  # Path to SFT LoRA adapters - will be merged into base model
attn_implementation: "flash_attention_2"
# LoRA configuration
lora: true
r: 32
lora_alpha: 64  # lora paper describes 2x the r
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"