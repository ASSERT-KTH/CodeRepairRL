# Model configuration
# For GRPO training, use the merged SFT model path instead of base model + LoRA checkpoint
# Example: model_name: "outputs/sft_model_merged" or "bjarni/qwen3-8b-swe-gym-sft-merged"
model_name: "Qwen/Qwen3-8B"
attn_implementation: "flash_attention_2"  #  flash attention 3 is only supported on >= H100

# LoRA configuration
# Having a lower rank LoRA is supported by literature, having low rank updates is actually beneficial to avoid overfitting / overspecializing
lora: true
r: 8
lora_alpha: 16  # lora paper describes 2x the r
target_modules:
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
layers_pattern: "blocks.{}"     # default for Qwen
layers_to_transform:            # last 1/3 of 32-layers
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31