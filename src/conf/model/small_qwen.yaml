# Model configuration
model_name: "Qwen/Qwen3-1.7B"
attn_implementation: "flash_attention_2"  # Unsloth typically manages attention. This may be ignored when using Unsloth.
load_in_4bit: true
unsloth_max_seq_length: null
# LoRA configuration
lora: true
lora_dropout: 0.0
# only used if run.lora is true
r: 32
lora_alpha: 64  # lora paper describes 2x the r
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"