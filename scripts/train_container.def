Bootstrap: docker
From: vllm/vllm-openai:v0.11.0

%environment
    # Keep container largely stateless; runtime env is passed via --env from host
    export SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt

%files
    src/
    pyproject.toml

%post
    apt-get update && \
    apt-get install -y --no-install-recommends git curl ca-certificates ripgrep
    
    pip install --upgrade pip
    pip install -e .
    
    # flash-attn is needed by transformers for attn_implementation="flash_attention_2" during training
    # (vLLM only bundles it for inference, not as an importable package)
    # Using --no-cache-dir to avoid cross-device link errors in container builds
    pip install --no-build-isolation --no-cache-dir flash-attn==2.8.3
    pip install --no-build-isolation --no-cache-dir flashinfer-python==0.5.0
    