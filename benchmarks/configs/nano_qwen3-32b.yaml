# Agent configuration
agent:
  kind: nano
  backend: apptainer
  
  # Hyperparameters
  token_limit: 65536
  time_limit: 600
  tool_limit: 500
  temperature: 1.0
  top_p: 0.95
  top_k: 20
  min_p: 0

# Model configuration
model:
  base_model: "Qwen/Qwen3-32B"  # HuggingFace model path
  scaffold: "nano-agent"  # Identifier for run tagging

# vLLM server configuration
vllm:
  command: |
    uv run vllm serve {BASE_MODEL} \
      --port {PORT} \
      --enable-auto-tool-choice \
      --tensor-parallel-size 8 \
      --max-model-len {MAX_CONTEXT_LEN} \
      --hf-overrides '{"max_position_embeddings": {MAX_CONTEXT_LEN}}' \
      --enable-prefix-caching \
      --reasoning-parser deepseek_r1 \
      --tool-call-parser hermes \
      {LORA_MODULES}
  
  # Environment variables for vLLM
  env:
    MAX_CONTEXT_LEN: 65536
    VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1

# Endpoint configuration
endpoint:
  provider: vllm
  
  # Base URL (for vLLM, this is constructed from port)
  base_url: "http://localhost:{PORT}/v1"
  
  # Model name format passed to agent
  # For vLLM: "hosted_vllm/{MODEL_NAME}"
  model_name_format: "hosted_vllm/{MODEL_NAME}"
  
  # API key (for external providers)
  api_key: null  # or "sk-..." for OpenAI/Mistral/etc.
  
  litellm_drop_params: true
  
  # Extra parameters to pass to litellm (e.g., enable_thinking for reasoning models)
  extra_params:
    enable_thinking: true

# Evaluation/Dataset configuration
eval:
  subset: verified
  split: test
  slice: null
  output_base_dir: "nano_qwen3-32b_swe-bench/"
  max_workers: 48

# Job script settings
job:
  port: 8000
  start_server: true